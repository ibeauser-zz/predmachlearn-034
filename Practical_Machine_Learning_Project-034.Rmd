---
title: "Practical Machine Learning Project"
author: "I. Beauser"
date: "November 2015"
output: html_document
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'index.html')) })
---
```{r 'switch',eval=TRUE,echo=FALSE,results='hide'}
#switch for modeling evaluation and caching
EVAL <- TRUE
CACHE <- FALSE
```
```{r 'setenviro', echo=FALSE,results='hide',warning=FALSE,message=FALSE,cache=FALSE}
## load necessary packages
if (!require(RCurl)) install.packages("RCurl"); library(RCurl)
if (!require(data.table)) install.packages("data.table"); library(data.table)
if (!require(randomForest)) install.packages("randomForest"); library(randomForest)
if (!require(e1071)) install.packages("e1071"); library(e1071)
if (!require(rpart)) install.packages("rpart"); library(rpart)
if (!require(gbm)) install.packages("gbm"); library(gbm)
if (!require(ipred)) install.packages("ipred"); library(ipred)
if (!require(proxy)) install.packages("proxy"); library(proxy)
if (!require(plyr)) install.packages("plyr"); library(plyr)
if (!require(dplyr)) install.packages("dplyr"); library(dplyr)
if (!require(tidyr)) install.packages("tidyr"); library(tidyr)
if (!require(caret)) install.packages("caret"); library(caret)
if (!require(doMC)) install.packages("doMC"); library(doMC)
## set computing environment
registerDoMC(cores = 4)
## get data
# data kindly provided by Groupware@LES
#trainURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#download.file(trainURL, destfile = "pml-training.csv")
#testURL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#download.file(testURL, destfile = "pml-testing.csv")
```
***

###Executive Summary
A file ([pml-training.csv](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)), comprising activity measurements of six subjects performing a dumbbell lift exercise, holds data from sensors on the subjects' body and dumbbell, as well as a classification of whether the participant lifted the dumbbell properly (Class 'A'), or otherwise (Classes 'B' through 'E').  The data was provided by Groupware@LES[[1]](#R1), and supported an academic paper[[2]](#R2) on Human Activity Recognition.

The Coursera Practical Machine Learning course project required a machine learning algorithm be built that could accurately predict activity quality from the activity monitor data derived from the Human Activity Recognition research.  To that end, various models were fit against a scrubbed training set, and predictions were made on a test set.

The best estimated Out-of-Sample accuracy measurements were produced by the Stochastic Gradient Boosting model, which was subsequently applied to the [scoring](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) dataset.  This report describes the development and evaluation process that led to the final model selection.

###The Data
The original data was reviewed by examining features with 100 or fewer distinct values.

``` {r 'reviewData',eval=TRUE,echo=FALSE,warning=FALSE,message=FALSE,cache=FALSE}
## read in review data
readIn <- fread("pml-training.csv")
## look for unusual character data
readIn[, rowid := .I]
setkey(readIn, rowid)
reviewData <- distinct(readIn[,names(readIn) := lapply(.SD, function(x) if(length(unique(x)) >= 100) {return(NULL)} else{return(x)})]) # columns with 100 or fewer distinct values, listing only those distinct values
```
```{r 'glimpse',eval=TRUE,echo=TRUE,warning=FALSE,message=FALSE,cache=FALSE}
glimpse(reviewData)
```

Many of the features contained spurious and/or incomplete data.  Of particular note, there appeared to be an association between the "new_window==yes" variable and those observations containing "#DIV/0!".  These were assumed to be summary rows and were removed from the training dataset.  Also of note, there were many "" values that needed to be converted to NA.  These were corrected at read time.  The dataset provided for prediction scoring, *pml-testing.csv* was left untouched by the load.

``` {r 'devDataset',eval=TRUE,echo=TRUE,cache=FALSE}
## read in training and testing(scoring) data
devDataset <- fread("pml-training.csv", na.strings = c("NA","N/A","null",""))
submitDataset <- fread("pml-testing.csv")
```

The "devDataset" data,

* was split into training and testing datasets,

```{r 'splits',eval=TRUE,echo=TRUE,warning=FALSE,message=FALSE,cache=FALSE}
set.seed(649)
inTrain <- createDataPartition(devDataset$classe, p = 0.8, list = FALSE)
training <- devDataset[inTrain]
testing <- devDataset[-inTrain]
```

* the observations that appear to contain summary values ("new.window" == "yes") were removed,

```{r 'summaries',eval=TRUE,echo=TRUE,warning=FALSE,message=FALSE,cache=FALSE}
inRemove <- lapply(training, function(x) which(x == "yes"))
removeRows <- inRemove[[names(which(rapply(inRemove, function(x) !is.null(unlist(inRemove[x])))))]]
training <- training[-removeRows,]
```

* the outcome variable ("classe") was moved to the first column, and, under the assumption that the predictors were sensor locations, only those variable variants were retained,

```{r 'outcome',eval=TRUE,echo=TRUE,warning=FALSE,message=FALSE,cache=FALSE}
training <- select(training, classe, matches("belt|arm|dumbbell|forearm"))
```

* the NA and near-NA (>=90%) predictors were removed from the training dataset,

```{r 'NAs',eval=TRUE,echo=TRUE,cache=FALSE}
training <- training[, which(unlist(lapply(training, function(x)(sum(!is.na(x))/NROW(x)) >= 0.9))), with = FALSE]
```

* the zero and near-zero variance predictors were removed from the training dataset, and

```{r 'zero',eval=TRUE,echo=TRUE,warning=FALSE,message=FALSE,cache=FALSE}
sensor.variables <- select(training, matches("belt|arm|dumbbell|forearm"))
nzv <- nearZeroVar(sensor.variables, saveMetrics = TRUE) # function from caret
training <- training[, c("classe",row.names(nzv[nzv$zeroVar == FALSE | nzv$nzv == FALSE,])), with = FALSE]
```

* the highly correlated (90% or better) predictors were identified and removed from the training dataset.

```{r 'correlated',eval=TRUE,echo=TRUE,warning=FALSE,message=FALSE,cache=FALSE}
corMatrix <- cor(training[,-"classe", with = FALSE])
highCorrPred <- findCorrelation(corMatrix, cutoff = 0.90, names = TRUE, exact = TRUE)
training <- training[, -highCorrPred, with = FALSE]
```

The final training dataset comprised `r dim(training)[[2]]-1` predictors and `r dim(training)[[1]]` observations.

###The Modeling
Based on a lecture note[[3]](#R3) written by instructor Jeff Leek, in which he states that random forests and boosting are typically among the top two performing algorithms in prediction contests, three modeling algorithms were assessed.  The first two, Random Forests and Stochastic Gradient Boosting, were chosen based on Leek's guidance.  The third, Bagged CART, chosen using the *caret* package dissimilarity code[[4]](#R4), provided a bagging-type model.

The Random Forests, Stochastic Gradient Boosting, and Bagged CART algorithms were implemented using the [caret package](http://topepo.github.io/caret/index.html) methods *rm*, *gbm*, and *treebag*, respectively.  The results were evaluated for accuracy, and the top performer was used to predict against the "submitDataset" for scoring.

Each of the models was built using 5-fold cross-validation.  The *caret* package has "the ability to adaptively resample the tuning parameter grid in a way that concentrates on values that are the in the neighborhood of the optimal settings"[[5]](#R5), so guidance was taken from the *caret* documentation for any extra model tuning required by the methods chosen.

```{r 'seeding',echo=FALSE,eval=TRUE,results='hide'}
## set seeds for training control
set.seed(649)
seeds <- vector(mode = "list", length = 6)
for (i in 1:5) seeds[[i]] <- sample.int(1000, 8)
seeds[[6]] <- sample.int(1000, 1)
```

####Random Forests Model
```{r 'rf',echo=TRUE,eval=EVAL,cache=CACHE,warning=FALSE,message=FALSE,results='hide'}
controlRF <- trainControl(method = "cv", number = 5, seeds = seeds)
tuneRF <- expand.grid(mtry = c(15,20,25,30))
set.seed(649)
fitRF <- train(classe ~ ., data = training, method = "rf", tuneGrid = tuneRF, trControl = controlRF, allowParallel = TRUE)
```
```{r 'showFitRF',echo=TRUE,eval=TRUE}
show(fitRF)
```

Five-fold cross-validation using the Random Forests model produced an in-sample error rate of `r round((1-max(fitRF$results$Accuracy))*100,3)`%, calculated from the model by

``` {r 'erRF',echo=TRUE,eval=FALSE}
1-max(fitRF$results$Accuracy)
```

The accuracy of the Random Forests model's predictions were estimated on the test dataset outcomes.

```{r 'predRF',echo=TRUE,eval=TRUE,cache=FALSE,warning=FALSE,message=FALSE}
predictRF <- predict(fitRF, newdata = testing)
cmRF <- confusionMatrix(predictRF, testing$classe)
```
```{r 'printRF',echo=FALSE,eval=TRUE,cache=FALSE,warning=FALSE,message=FALSE}
print("Random Forests Confusion Matrix")
cmRF
```

The resultant estimated out-of-sample (OOS) error rate for the Random Forests model was **1- Accuracy = `r round((1-cmRF$overall[[1]])*100,3)`%**, calculated from the prediction by

```{r 'oosRF',echo=TRUE,eval=FALSE}
1-cmRF$overall[[1]]
```

####Boosting Model
```{r 'gbm',echo=TRUE,eval=EVAL,cache=CACHE,warning=FALSE,message=FALSE,results='hide'}
controlGMB <- trainControl(method = "cv", number = 5, seeds = seeds)
tuneGBM <- expand.grid(n.trees = seq(1,501,10), interaction.depth = c(1, 5, 9), shrinkage = 0.1, n.minobsinnode = 20)
set.seed(649)
fitGBM <- train(classe ~ ., data = training, method = "gbm", tuneGrid = tuneGBM, trControl = controlGMB)
```

Similarly, the in-sample error rate of the Stochastic Gradient Boosting model was calculated as `r round((1-max(fitGBM$results$Accuracy))*100,3)`%.

The Confusion Matrix derived from applying the model to the test dataset was:

```{r 'predGBM',echo=FALSE,eval=TRUE,cache=FALSE,warning=FALSE,message=FALSE}
predictGBM <- predict(fitGBM, newdata = testing)
cmGBM <- confusionMatrix(predictGBM, testing$classe)
```
```{r 'printGBM',echo=FALSE,eval=TRUE,cache=FALSE,warning=FALSE,message=FALSE}
print("Stochastic Gradient Boosting")
cmGBM
```

The resultant estimated out-of-sample (OOS) error rate for the Boosting model was **`r round((1-cmGBM$overall[[1]])*100,3)`%**, calculated in the same manner as Random Forests, above.

####Bagging Model
```{r 'treebag',echo=TRUE,eval=EVAL,cache=CACHE,warning=FALSE,message=FALSE,results='hide'}
controlTBAG <- trainControl(method = "cv", number = 5, seeds = seeds)
set.seed(649)
fitTBAG <- train(classe ~ ., data = training, method = "treebag", trControl = controlTBAG)
```
Using the same methodology employed in the two models above, the in-sample error rate for the Bagged CART model was calculated as `r round((1-max(fitTBAG$results$Accuracy))*100,3)`%.

```{r 'predTBAG',echo=FALSE,eval=TRUE,cache=FALSE,warning=FALSE,message=FALSE}
predictTBAG <- predict(fitTBAG, newdata = testing)
cmTBAG <- confusionMatrix(predictTBAG, testing$classe)
```

The Confusion Matrix (not shown), provided an estimated OOS error rate of **`r round((1-cmTBAG$overall[[1]])*100,3)`%** from the Bagged CART model's predictions.

###Results
The accuracy results from the three models were tabulated and graphed, see below.

```{r 'results',echo=FALSE,eval=TRUE}
results <- resamples(list(RandomForest = fitRF, Boosting = fitGBM, Bagging = fitTBAG),decreasing=TRUE)
summary(results)
bwplot(results)
```

The computational time for all three models was also calculated.

```{r 'comptime',echo=FALSE,eval=TRUE}
timings <- results$timings
timings
```

###Conclusion
The Bagging model had the lowest accuracy, and thus was set aside.  Boosting had a slightly better estimated accuracy over Random Forests, but the difference is small.  If computational efficiency was to be taken into account, the Random Forests model would be preferred.  For the purposes of this project, however, the best estimated performer, Stochastic Gradient Boosting, was the accepted modeling algorithm, and it was used for predictive scoring.

###References

1. <a id="R1"></a> Groupware@LES Human Activity Recognition Project [http://groupware.les.inf.puc-rio.br/har](http://groupware.les.inf.puc-rio.br/har)
2. <a id="R2"></a>Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013 Read [more](http://groupware.les.inf.puc-rio.br/har#ixzz3rgxKzkqm).
3. <a id="R3"></a>Jeff Leek. Random Forests [lecture](http://datasciencespecialization.github.io/courses/08_PracticalMachineLearning/021randomForests/#10), "Practical Machine Learning", Coursera/John Hopkins Bloomberg School of Public Health 2015.
4. <a id="R4"></a>Max Kuhn, The **caret** Package. ["Models Clustered by Tag Similarity"](http://topepo.github.io/caret/similarity.html) 2015.
5. <a id="R5"></a>Max Kuhn, The **caret** Package. ["Adaptive Resampling"](http://topepo.github.io/caret/adaptive.html)

###Addendum

The Stochastic Gradient Boosting model developed above achieved a 20/20 result on the prediction scoring test.
***
  
&copy; Copyright 2015 I. Beauser, All rights reserved.
